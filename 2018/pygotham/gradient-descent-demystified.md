# "Gradient Descent, Demystified" by Michael (Stu) Stewart

Gradient descent finds the minimum of a function iteratively by following the contour of some gradient.


## An easy case

The parabola x^2 is easy to find the minimum of (0).
If `x < 0`, moving in the positive x direction gets us closer to 0.
If `x > 0`, moving in the negative x direction gets us closer to 0.


## Linear regression

For linear regression, gradient descent is also not necessary;
you can solve the _normal equation_ and determine the beta values.
Use the identity function to solve for linear regression.


## Logistic regression

Use the sigmoid function to solve for logistic regression.
